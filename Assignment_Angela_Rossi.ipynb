{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "xkli5kwU3k-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7g3j6uNuypmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6af425-8134-407a-fb89-9305d223f1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (60000, 28, 28, 1)\n",
            "Test data shape: (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    Flatten,\n",
        "    Reshape,\n",
        "    LeakyReLU\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the data\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "\n",
        "# Convert to float32 and normalize\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Add channel dimension\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Class"
      ],
      "metadata": {
        "id": "yIXSncgF3qsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded = self.encoder(inputs)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "h7Aeuvzk3t8Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurations and Architecture Variations"
      ],
      "metadata": {
        "id": "Q2QiQwa433g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = [\n",
        "    {\n",
        "        'name': 'baseline',\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'optimizer': Adam(learning_rate=0.001),\n",
        "        'activation': 'relu',\n",
        "        'latent_dim': 2,\n",
        "        'architecture': 'standard',\n",
        "        'regularization': None\n",
        "    },\n",
        "    {\n",
        "        'name': 'deep_architecture',\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'optimizer': Adam(learning_rate=0.001),\n",
        "        'activation': 'relu',\n",
        "        'latent_dim': 2,\n",
        "        'architecture': 'deep',\n",
        "        'regularization': None\n",
        "    },\n",
        "    {\n",
        "        'name': 'wide_architecture',\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'optimizer': Adam(learning_rate=0.001),\n",
        "        'activation': 'relu',\n",
        "        'latent_dim': 2,\n",
        "        'architecture': 'wide',\n",
        "        'regularization': None\n",
        "    },\n",
        "    {\n",
        "        'name': 'leaky_relu',\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'optimizer': Adam(learning_rate=0.001),\n",
        "        'activation': 'leaky_relu',\n",
        "        'latent_dim': 2,\n",
        "        'architecture': 'standard',\n",
        "        'regularization': None\n",
        "    },\n",
        "    {\n",
        "        'name': 'selu',\n",
        "        'epochs': 50,\n",
        "        'batch_size': 128,\n",
        "        'optimizer': Adam(learning_rate=0.001),\n",
        "        'activation': 'selu',\n",
        "        'latent_dim': 2,\n",
        "        'architecture': 'standard',\n",
        "        'regularization': None\n",
        "    }\n",
        "]  # Reduced number of configurations for faster testing\n"
      ],
      "metadata": {
        "id": "kex4BDrg37hC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "yssrPJ6H4lsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(activation_name):\n",
        "    if activation_name == 'leaky_relu':\n",
        "        return tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "    return activation_name\n",
        "\n",
        "def create_encoder(config, input_shape):\n",
        "    encoder = Sequential()\n",
        "\n",
        "    # Get regularizer if specified\n",
        "    reg = None\n",
        "    if config['regularization'] == 'l1':\n",
        "        reg = l1(0.01)\n",
        "    elif config['regularization'] == 'l2':\n",
        "        reg = l2(0.01)\n",
        "\n",
        "    activation = get_activation(config['activation'])\n",
        "\n",
        "    if config['architecture'] == 'standard':\n",
        "        encoder.add(Conv2D(48, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, input_shape=input_shape,\n",
        "                          kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(96, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(96, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(96, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    elif config['architecture'] == 'deep':\n",
        "        encoder.add(Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, input_shape=input_shape,\n",
        "                          kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(64, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(128, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(256, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    elif config['architecture'] == 'wide':\n",
        "        encoder.add(Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
        "                          activation=activation, input_shape=input_shape,\n",
        "                          kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(256, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "        encoder.add(Conv2D(256, kernel_size=3, strides=2, padding='same',\n",
        "                          activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    enc_shape = encoder.output_shape[1:]\n",
        "    encoder.add(Flatten())\n",
        "    encoder.add(Dense(units=config['latent_dim'], name='latent_space',\n",
        "                     kernel_regularizer=reg))\n",
        "\n",
        "    return encoder, enc_shape\n",
        "\n",
        "def create_decoder(config, enc_shape, latent_dim):\n",
        "    decoder = Sequential()\n",
        "\n",
        "    reg = None\n",
        "    if config['regularization'] == 'l1':\n",
        "        reg = l1(0.01)\n",
        "    elif config['regularization'] == 'l2':\n",
        "        reg = l2(0.01)\n",
        "\n",
        "    activation = get_activation(config['activation'])\n",
        "\n",
        "    decoder.add(Dense(units=np.prod(enc_shape), input_shape=(latent_dim,),\n",
        "                     kernel_regularizer=reg))\n",
        "    decoder.add(Reshape(enc_shape))\n",
        "\n",
        "    if config['architecture'] == 'standard':\n",
        "        decoder.add(Conv2DTranspose(96, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(96, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(48, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    elif config['architecture'] == 'deep':\n",
        "        decoder.add(Conv2DTranspose(256, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(128, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(32, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    elif config['architecture'] == 'wide':\n",
        "        decoder.add(Conv2DTranspose(256, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(256, kernel_size=3, strides=2, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "        decoder.add(Conv2DTranspose(128, kernel_size=3, strides=1, padding='same',\n",
        "                                  activation=activation, kernel_regularizer=reg))\n",
        "\n",
        "    decoder.add(Conv2DTranspose(1, kernel_size=3, strides=1, padding='same',\n",
        "                               activation='sigmoid'))\n",
        "\n",
        "    return decoder\n",
        "\n",
        "def train_autoencoder(config, X_train, X_test):\n",
        "    encoder, enc_shape = create_encoder(config, X_train.shape[1:])\n",
        "    decoder = create_decoder(config, enc_shape, config['latent_dim'])\n",
        "\n",
        "    autoencoder = Autoencoder(encoder, decoder)\n",
        "    autoencoder.compile(optimizer=config['optimizer'], loss='mse')\n",
        "\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    history = autoencoder.fit(\n",
        "        X_train, X_train,\n",
        "        epochs=config['epochs'],\n",
        "        batch_size=config['batch_size'],\n",
        "        validation_data=(X_test, X_test),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return autoencoder, history\n",
        "\n",
        "def plot_results(histories):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history.history['loss'], label=f'{name} (train)')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history.history['val_loss'], label=f'{name} (val)')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_reconstructions(autoencoder, X_test, n=10):\n",
        "    indices = np.random.randint(0, X_test.shape[0], n)\n",
        "    test_images = X_test[indices]\n",
        "    reconstructions = autoencoder.predict(test_images)\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Original')\n",
        "\n",
        "        plt.subplot(2, n, n + i + 1)\n",
        "        plt.imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pC2o-d934sXP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Experiments"
      ],
      "metadata": {
        "id": "52truWS-5ukJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {}\n",
        "autoencoders = {}\n",
        "\n",
        "for config in configurations:\n",
        "    print(f\"\\nTraining configuration: {config['name']}\")\n",
        "    autoencoder, history = train_autoencoder(config, X_train, X_test)\n",
        "    histories[config['name']] = history\n",
        "    autoencoders[config['name']] = autoencoder\n",
        "\n",
        "# Plot results\n",
        "plot_results(histories)\n",
        "\n",
        "# Visualize reconstructions for each model\n",
        "for name, autoencoder in autoencoders.items():\n",
        "    print(f\"\\nReconstructions for {name}\")\n",
        "    visualize_reconstructions(autoencoder, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCJkeIS-5xvB",
        "outputId": "a70d9d2b-0b63-4215-d506-8d03f703a2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training configuration: baseline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 762ms/step - loss: 0.0861 - val_loss: 0.0498 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 755ms/step - loss: 0.0477 - val_loss: 0.0436 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 764ms/step - loss: 0.0432 - val_loss: 0.0415 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 758ms/step - loss: 0.0414 - val_loss: 0.0405 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m 36/469\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:08\u001b[0m 713ms/step - loss: 0.0406"
          ]
        }
      ]
    }
  ]
}